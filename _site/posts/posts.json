[
  {
    "path": "posts/2024-02-07-ml-r-python/",
    "title": "Machine Learning with R and Python",
    "description": "KNN and Random Forest Regression Modeling",
    "author": [
      {
        "name": "Jess Hooker",
        "url": {}
      }
    ],
    "date": "2024-02-04",
    "categories": [],
    "contents": "\r\nOne of my personal goals has been to advance my analytic skills in both Python and R. I am still partial to R because I use it regularly for work and it is a powerhouse for statistical analysis, but the consistency and flexibility of Python is winning me over. I have been steadily grinding away at my master’s in data analytics at WGU over the past 6-months and my latest projects have focused on machine learning algorithms for continuous and categorical outcomes. I decided to use one tool for each project. Both projects were primarily for skill demonstration and utilized practice datasets comprising 10,000 rows and 50 variables. After running each of them, I found both programming languages to be straightforward in their implementation, though, I would argue the pre-processing steps go much more quickly in Python. Below are brief descriptions of each project, with links to the source code on my GitHub.\r\nProject 1: Predicting Customer Churn with K-Nearest Neighbors\r\nThe primary goal of this project was to build a k-nearest neighbors (KNN) algorithm that accurately predicted whether a customer has churned from 25 customer characteristics and service features using data from a “telecommunication company.” I conducted this project in R. Prior to building the KNN model, I used recursive feature elimination with random forests and 10-fold cross-validation to narrow the features down to 8. Then, using the caret package, I performed hyperparameter tuning with cross-fold validation to identify optimal values of k.\r\n\r\n\r\n\r\nFigure 1: This plot shows the performance of different values of k tested during hyperparameter tuning with cross-validation.\r\n\r\n\r\n\r\nWith an optimal k value of 19, the final model achieved 90% accuracy on a held-out data subset. Below is the ROC curve for the final model. The code for this project can be found  here.\r\n\r\n\r\n\r\nFigure 2: This is the ROC curve for the final KNN model on the held-out test data.\r\n\r\n\r\n\r\nProject 2: Predicting Hospitalization Length with Random Forest\r\nThe primary goal of this project was to use random forest regression to predict patients’ length of hospitalization from their demographic, health, and treatment characteristics. I conducted this project with Python using Jupyter Notebook. For this analysis, I primarily relied on the scikit-learn library for pre-processing, hyperparameter tuning, and model building. Below is a snapshot of the variable importance estimates from the training model. Overall, this model had very poor predictive power, with only marginal improvement over the baseline model. Sometimes there aren’t relationships! Though, this is likely a facet of this practice data set. If this had been real patient data, I am confident the model would have predicted hospitalization length with a lot better accuracy. The source code for this project can be found  here.\r\n\r\n\r\n\r\nFigure 3: This plot shows the aggregrated variable importance estimates for each variable included in the random forest regression model using the training data set.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-02-07-ml-r-python/knn_tuning.png",
    "last_modified": "2024-02-07T14:36:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-07-13-wa-electric-cars/",
    "title": "Electric Cars in Washington State",
    "description": "A Dashboard.",
    "author": [
      {
        "name": "Jess Hooker",
        "url": {}
      }
    ],
    "date": "2023-07-13",
    "categories": [],
    "contents": "\r\nThis was an exploratory project examining all of the electric vehicles currently registered in the state of Washington. I obtained the data set from the Department of Licensing here. The data set comprises just over 138,000 registered vehicles with information that includes the make, model, year, and the number of miles that can be driven on electric power only. Using Tableau, I created a dashboard visualizing several features including the the proportion of registered cars by make and the top 10 electric car cities. This was a fun project exploring the capabilities of Tableau and the ability to quickly create an informative dashboard.\r\nI was surprised to see Nissan represented such a large portion of the registered vehicles at almost 10%. I was also surprised to see how much of a majority Tesla vehicles represented, but after observing how far they can travel on electric power only, it makes sense. As expected, Seattle has the most number of registered electric vehicles at 17% of all registered vehicles. Another observation I made while creating this dashboard, was that while most of the vehicles were made in the list 5 years, there were a handful that date back all the way to the late 90s. That’s impressive!\r\nYou can see a snapshot of the dashboard believe or access the interactive version here on Tableau Public.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-13-wa-electric-cars/dashboard.png",
    "last_modified": "2023-07-13T17:30:00-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-14-sdoh-cardio-choropleths/",
    "title": "2019 US Cardiovascular Health Indicators",
    "description": "Mapping county-level health variables.",
    "author": [
      {
        "name": "Jess Hooker",
        "url": {}
      }
    ],
    "date": "2023-06-14",
    "categories": [],
    "contents": "\r\nThis project visually explores three indicators of cardiovascular health in adults across the US:\r\nAvoidable deaths due to heart disease and stroke\r\nPrevalence of reported high cholesterol\r\nPrevalence of reported lack of exercise\r\nThe data is from the Agency for Healthcare Research and Quality Social Determinants of Health (SDOH) Database for the year 2019. The SDOH comprises data pulled from several sources across five core domains: social context, economic context, education, physical infrastructure, and healthcare context.\r\nI used choropleth maps to visualize data drawn from two Centers for Disease Control and Prevention sources within the the SDOH Database-the Interactive Atlas of Heart Disease and Stroke and the Population Level Analysis and Community Estimates: Local Data for Better Health. I completed this project in Jupyter Notebooks with the Plotly library. You can find the source code for this project on my GitHub.\r\n\r\n\r\n\r\nFigure 1: This plot represents the total number of avoidable deaths due to heart disease and stroke per 100,000 people by county. An ‘avoidable death’ is one that occurs in an adult under the age of 75. White spaces represent counties for which data was not available.\r\n\r\n\r\n\r\nOf the three variables of interest, the range for this indicator is the largest. Yet, there is a clear pattern of estimates at the lower end of the range for a majority of the country. This likely indicates some skewness in the data, with a smaller number of extreme values at the upper end of the range (indicated by counties in red). Compared to other regions of the US, there is a pattern of a higher number of avoidable deaths in the southeastern counties. The highest concentration of a high number of avoidable deaths, based on visual examination, appears to be in Oklahoma.\r\n\r\n\r\n\r\nFigure 2: This plot represents the prevalence of adults over the age of 18 who report they have high cholesterol accordnig to a doctor or health professional. White spaces represent counties for which data was not available.\r\n\r\n\r\n\r\nThis variable has the smallest range of values. Despite reduced variability in the range for this indicator across counties overall, there is a clear pattern of higher prevalence in the southeast US (with the exception of Florida). States that appear to have lower prevalence rates across counties are relatively concentrated in the northern states of the western US, as well as New Hampshire.\r\n\r\n\r\n\r\nFigure 3: This plot represents the prevalence of adults over the age of 18 who report not participating in physical activities or exercise outside of their job. Note: White spaces represent counties for which data was not available.\r\n\r\n\r\n\r\nSome examples of physical activities include running, walking for exercise, golf or gardening. The range of prevalence estimates here are slightly larger than the range for the cholesterol variable above. As with previous visualizations, there is a concentration of higher prevalence of no exercise in the southeast, particularly in Mississippi counties. Interestingly, there are several counties in South Dakota with high rates of non-exercise, but these same counties have relatively low rates of both adults with high cholesterol and avoidable deaths due to heart disease and stroke.\r\nSummary\r\nThe visualizations presented above provide a broadband overview of the health of American adults based on three indicators related to cardiovascular health and physical exercise. While the data are presented in different scales, preliminary examinations of the graphs highlight geographic consistency for regions with poorer health outcomes; the southeastern US in particular. While these graphics do not directly imply a causal link between exercise and cardiovascular outcomes, physical exercise is a known modifiable risk factor for cardiovascular health. It is important to note that these graphics provide a population-level overview of these indicators and do not capture factors that may explain regional differences.\r\nSources:\r\nArea Health Resources Files (AHRF) 2009-2019. US Department of Health and Human Services, Health Resources and Services Administration, Bureau of Health Workforce, Rockville, MD\r\nAHRQ SDOH Database (v1), from Centers for Disease Control and Prevention, Division for Heart Disease and Stroke Prevention, Interactive Atlas of Heart Disease and Stroke\r\nAHRQ SDOH Database (v1), from PLACES: Local Data for Better Health. Center for Disease Control and Prevention (2020 and 2021 Release)\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-06-14-sdoh-cardio-choropleths/avoidable_deaths.png",
    "last_modified": "2023-06-14T14:54:07-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-25-goodreads-database/",
    "title": "Exploring My Goodreads Data",
    "description": "Creating a Database",
    "author": [
      {
        "name": "Jess Hooker",
        "url": {}
      }
    ],
    "date": "2023-04-25",
    "categories": [],
    "contents": "\r\nThis is the first stage of a project exploring my reading activity from Goodreads. For the initial part of the project, I obtained three raw data from Goodreads:\r\nA CSV file of all of my books on my Goodreads shelves\r\nA JSON object of the activity posted to my Goodreads feed\r\nA JSON object of my book reviews\r\nI utilized Python to read the data files and create a database in MySQL. You can find the Jupyter Notebook for the data upload here. Next steps are to clean and process the data for exploratory analysis and visualizing.\r\nA snapshot of some of the syntax for creating a table in the database using Python.\r\n\r\n# create the activities table\r\ntry:\r\n    conn = mysql.connect(host='', database = 'goodreads', user = '', password = '')\r\n    if conn.is_connected():\r\n        cursor = conn.cursor()\r\n        cursor.execute(\"select database();\")\r\n        record = cursor.fetchone()\r\n        print(\"You're connect to the database:\", record)\r\n        cursor.execute('DROP TABLE IF EXISTS activities;')\r\n        print('Creating table...')\r\n# create the new table\r\n        cursor.execute('''\r\n            CREATE TABLE activities(\r\n                           activity_id int primary key not null,\r\n                           activity_type varchar(255),\r\n                           activity_desc varchar(255),\r\n                           product varchar(255),\r\n                           created_date varchar(255) \r\n                           )''')\r\n        print('activities table is created')\r\n# parse the json data\r\n    for i, item in enumerate(activities_list):\r\n        activity_id = i\r\n        activity_type = item.get('activity_type')\r\n        activity_desc = item.get('activity')\r\n        product = item.get('product')\r\n        created_date = item.get('created_at')\r\n    # pull the data into the database    \r\n        cursor.execute(\r\n                '''\r\n                INSERT INTO goodreads.activities (\r\n                activity_id,\r\n                activity_type,\r\n                activity_desc,\r\n                product,\r\n                created_date \r\n                )\r\n                VALUES (%s,%s,%s,%s,%s)\r\n                ''', \r\n                (\r\n                activity_id,\r\n                activity_type,\r\n                activity_desc,\r\n                product,\r\n                created_date,\r\n              )                \r\n            )\r\n        print(\"record inserted\")\r\n    conn.commit()\r\nexcept Error as e:\r\n    print(\"Error while connecting to MySQL\", e)       \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-25-goodreads-database/goodreads_sql_preview.png",
    "last_modified": "2023-04-25T21:15:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-31-us-wealth/",
    "title": "A Look at US Wealth",
    "description": {},
    "author": [
      {
        "name": "Jess Hooker",
        "url": {}
      }
    ],
    "date": "2023-03-31",
    "categories": [],
    "contents": "\r\nThis was collaborative project exploring wealth data from the US Census Bureau. We have created a series of visualizations exploring the distribution of US wealth across wealth percentile groups, income percentile groups, education levels, and generation. These visuals were created with quarterly data from the third quarter of 1989 to the third quarter of 2022 gathered by the US Census Bureau. You can view the final project here.\r\nIn each visualizations we explored key characteristics contributing to wealth in the US over the last 33 years. All together, it’s clear the college-educated and those in the Baby Boomer generation consistently held a significant proportion of the wealth in this country. We also observed large disparities in wealth when it comes to examining both income and wealth percentiles, with those in the 50th percentile and below holding less 15% of nation’s wealth across both metrics.\r\n\r\n\r\n\r\nFigure 1: This plot represents US wealth in trillions of dollars from 1989 to 2022. Each color represents a generation in addition to total wealth of the US. As we can see, the rate of change in wealth over time is not equitable across generations, with the Silent and Earlier and Millennial Generations showing relatively stable amounts of wealth over time. This is likely related to age, as the Silent and Earlier generation has aged into retirement during this period while the Millennial have only recently reached young to middle adulthood. The Baby Boomer and Gen X show very similar patterns of growth over time.\r\n\r\n\r\n\r\nTo create this project, we used Jupyter Notebooks to process the data and develop the visualizations. Then, I used python within R Markdown to design and render this page as HTML. You can find the source code for this project on GitHub.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-31-us-wealth/wealth_preview.png",
    "last_modified": "2023-04-25T21:15:47-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-22-watereda/",
    "title": "Exploratory Analysis of Water Samples",
    "description": {},
    "author": [
      {
        "name": "Jess Hooker",
        "url": {}
      }
    ],
    "date": "2022-12-14",
    "categories": [],
    "contents": "\r\nPredicting the Potability of Water\r\nThis is the first part of an exploratory data analysis project examining characteristics of about 3000 water samples in relation to potability. I obtained this data project as part of the beta testing of the Lonely Octopus. It comprises 9 primary water sampling characteristics and a binary outcome indicating the sample was potable or not. In the first stage of this project, I focused utilizing loops in python to visualize the data. You can preview the results here and find the Jupyter Notebook syntax on GitHub.\r\n\r\n\r\n\r\nFigure 1: A correlation plot of each of the 9 water potability characteristics.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-22-watereda/water_eda.png",
    "last_modified": "2023-04-25T21:16:06-04:00",
    "input_file": {}
  }
]
